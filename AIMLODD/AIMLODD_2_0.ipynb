{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgJPRy3OYDRv/WygUz0pqJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Intro to Computer Vision\n",
        "\n",
        "References\n",
        "\n",
        "> AI and Machine Learning for On Device Development - Laurence Moroney\n",
        "\n",
        "> * https://www.oreilly.com/library/view/ai-and-machine/9781098101732/\n",
        "\n",
        "> * https://github.com/lmoroney/odmlbook/tree/main/BookSource\n",
        "\n",
        "> Fashion MNIST\n",
        "\n",
        "> * https://arxiv.org/abs/1708.07747\n",
        "\n",
        "> * https://paperswithcode.com/dataset/fashion-mnist\n",
        "\n",
        "> Keras\n",
        "\n",
        "> * https://keras.io/\n",
        "\n",
        "> Tensorflow\n",
        "\n",
        "> * https://www.tensorflow.org/\n",
        "\n",
        "\n",
        "IDE (Interactive Development Environment)\n",
        "\n",
        ">[Colab](https://colab.research.google.com)"
      ],
      "metadata": {
        "id": "Rw6NZBe5WRMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to look at a script that uses computer vision to classify clothing\n",
        "\n",
        "> * Fashion MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits."
      ],
      "metadata": {
        "id": "w82jaNwmNsbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "data = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()\n",
        "\n",
        "training_images  = training_images / 255.0\n",
        "val_images = val_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential(\n",
        "[tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        " tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "     tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "\n",
        "model.evaluate(val_images, val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MiemuZhPTtx",
        "outputId": "63f2de35-91e4-492e-ceb5-5b3cab141a48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.4036 - accuracy: 0.8871\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2318 - accuracy: 0.9334\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1952 - accuracy: 0.9444\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1719 - accuracy: 0.9503\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1532 - accuracy: 0.9554\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1406 - accuracy: 0.9591\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1303 - accuracy: 0.9620\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1222 - accuracy: 0.9646\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1164 - accuracy: 0.9653\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1110 - accuracy: 0.9669\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1061 - accuracy: 0.9686\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1014 - accuracy: 0.9704\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0976 - accuracy: 0.9703\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0946 - accuracy: 0.9716\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0920 - accuracy: 0.9725\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0882 - accuracy: 0.9735\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0864 - accuracy: 0.9743\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0836 - accuracy: 0.9751\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0814 - accuracy: 0.9750\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0794 - accuracy: 0.9756\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.9608\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.14016012847423553, 0.9607999920845032]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at each of the lines of code"
      ],
      "metadata": {
        "id": "MNa_y14eSv7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line imports the python libraries that we will need to run the script\n",
        "\n",
        "> https://www.tensorflow.org/"
      ],
      "metadata": {
        "id": "fSBJnN82S7dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "8g6eT2c4TA2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line loads the Fashion MNIST database, which we will evaluate, via tensorflow and keras"
      ],
      "metadata": {
        "id": "O9QjPpy4TQT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()"
      ],
      "metadata": {
        "id": "3rFhkYEvTWC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ILeVtm9VUDdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line scales down the pixels in the 60,000 training images, and the process is called normalization\n",
        "\n",
        "> Pixels normally have a range of values from 0 to 255\n",
        "\n",
        "> We are scaling that value range down to 0 to 1\n",
        "\n",
        "> A smaller pixel value range works well with the activation functions that we use in the neurons (nodes)\n",
        "\n",
        "> * errors do not massively inflate when using smaller pixel values"
      ],
      "metadata": {
        "id": "3OLh0H1ETlYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_images  = training_images / 255.0"
      ],
      "metadata": {
        "id": "uxc-W0poTqmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line scales down the pixels in the 10,000 validation (test) images, and the process is called normalization\n",
        "\n",
        "> Pixels normally have a range of values from 0 to 255\n",
        "\n",
        "> We are scaling that range down to 0 to 1\n",
        "\n",
        "> A smaller pixel range works well with the activation functions that we use in the neurons (nodes)\n",
        "\n",
        "> * errors do not massively inflate when using smaller pixel values"
      ],
      "metadata": {
        "id": "H3m5_AqFUIvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_images = val_images / 255.0"
      ],
      "metadata": {
        "id": "JlGX7ls4UKW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line defines the model architecture\n",
        "\n",
        "> Sequential()\n",
        "\n",
        "> * A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "\n",
        "> .Flatten()\n",
        "\n",
        "> * Flattens the input. Does not affect the batch size.\n",
        "\n",
        "> * https://keras.io/api/layers/reshaping_layers/flatten/\n",
        "\n",
        "> .Dense()\n",
        "\n",
        "> * Just your regular densely-connected NN layer\n",
        "\n",
        "> * https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "> nn.relu\n",
        "\n",
        "> * activation function - mimics the firing thresholds of a biological neuron\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/nn/relu\n",
        "\n",
        "> nn.softmax\n",
        "\n",
        "> * activation function - mimics the firing thresholds of a biological neuron\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/nn/softmax"
      ],
      "metadata": {
        "id": "LTLM4clHUxZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "[tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        " tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "     tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "metadata": {
        "id": "xdd7ajAyUyi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line configures the model for training\n",
        "\n",
        "> .compile()\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
        "\n",
        "> optimizer - algorithm selections\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "\n",
        "> loss - between true labels and predicted labels\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "\n",
        "> metrics - List of various metrics to be evaluated by the model during training and testing\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
      ],
      "metadata": {
        "id": "jYxOGUUaWmQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AcefpbGyWnZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line trains the model for a fixed number of epochs (dataset iterations) using the .fit method\n",
        "\n",
        "> * https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "\n",
        "> * https://keras.io/api/models/model_training_apis/\n",
        "\n",
        "> * a loop that makes a guess, measures how good or how bad that loss is, optimizes for a new guess, and repeats\n",
        "\n",
        "> *  epochs = 20, indicates that we’ll repeat the loop 20 times\n",
        "\n",
        "> TensorFlow supports batching to make things faster\n",
        "\n",
        "> * Fashion MNIST defaults to each batch having 32 images, so it trains with one batch of images at a time\n",
        "\n",
        "> * This gives you 1875 batches of images to make up 60,000 images (i.e., 60,000 divided by 32 = 1875)\n"
      ],
      "metadata": {
        "id": "Ua4MYHsqXAdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_images, training_labels, epochs=20)"
      ],
      "metadata": {
        "id": "AHvM6LcZXHLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line passes the 10,000 validation (test) images to the model to see how it accurately it parses (classifies, labels) them\n",
        "\n",
        "> * 313/313 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.9608\n",
        "[0.14016012847423553, 0.9607999920845032]\n",
        "\n",
        "> * our output reflects a 96% classification (labeling) accuracy with respect to assigning clothing labels to clothing images that it has not seen before\n",
        "\n",
        "> * we should always keep in mind the mathematical concept of overfitting as we evaulate our inputs and outputs ... GIGO"
      ],
      "metadata": {
        "id": "8_QnF9KwXfD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_images, val_labels)"
      ],
      "metadata": {
        "id": "AC1CzB0aXlLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
