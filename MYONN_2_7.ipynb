{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF3Ik3wsVwYrSRALfEMOKd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2.7 Neural Network Class - MNIST Test (July 2023)\n",
        "\n",
        "We will give the neural network a handwritten number, that it has not seen before, to identify\n",
        "\n",
        "References\n",
        "\n",
        "> Make Your Own Neural Network by Tariq Rashid\n",
        "\n",
        "> https://github.com/makeyourownneuralnetwork\n",
        "\n",
        "> Matplotlib\n",
        "\n",
        "> https://matplotlib.org\n",
        "\n",
        "> Numpy\n",
        "\n",
        "> https://numpy.org\n",
        "\n",
        "> Pandas\n",
        "\n",
        "> https://pandas.pydata.org\n",
        "\n",
        "> Python\n",
        "\n",
        "> https://www.python.org\n",
        "\n",
        "> https://docs.python.org/3/library/csv.html\n",
        "\n",
        "> Scipy\n",
        "\n",
        "> https://scipy.org\n",
        "\n",
        "> Wikipedia\n",
        "\n",
        "> https://en.wikipedia.org/wiki/MNIST_database\n",
        "\n",
        "> https://en.wikipedia.org/wiki/Normalization_(image_processing)\n",
        "\n",
        "> https://en.wikipedia.org/wiki/Spatial_anti-aliasing\n",
        "\n",
        "IDE\n",
        "> Google Colab\n",
        "\n",
        "> https://colab.research.google.com\n"
      ],
      "metadata": {
        "id": "Ddz01PEZVIId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "A class is a reuseable blueprint for creating objects.\n",
        "\n",
        "Our draft class will simulate a biological neural network by having three parts that serve to:\n",
        "\n",
        "> initialize - set quantity of input, hidden, & output nodes\n",
        "\n",
        "> train - refine network weights by using training data\n",
        "\n",
        "> query - given input data, provide an answer from the output nodes"
      ],
      "metadata": {
        "id": "fW92NZRQFCN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the draft class that we will use for recognizing hand-written numbers via neural network"
      ],
      "metadata": {
        "id": "g6_UBGC1orS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.special\n",
        "\n",
        "# draft class definition for a neural network\n",
        "class neuralNetwork:\n",
        "\n",
        "  # intialize the neural network\n",
        "  def __init__(self, inputNodes, hiddenNodes, outputNodes, learningRate):\n",
        "    # layers and learning rates\n",
        "    self.iNodes = inputNodes\n",
        "    self.hNodes = hiddenNodes\n",
        "    self.oNodes = outputNodes\n",
        "    self.learnRate = learningRate\n",
        "    # link weights connecting the layers via matrices\n",
        "    self.wih = np.random.normal(0.0, pow(self.iNodes, -0.5), (self.hNodes, self.iNodes))\n",
        "    self.who = np.random.normal(0.0, pow(self.hNodes, -0.5), (self.oNodes, self.hNodes))\n",
        "    # sigmoid activation function\n",
        "    self.activation_function = lambda x: scipy.special.expit(x)\n",
        "    pass\n",
        "\n",
        "  # train the neural network\n",
        "  def train(self, inputs_list, targets_list):\n",
        "    # convert the inputs list to a 2d array\n",
        "    inputs = np.array(inputs_list, ndmin = 2).T\n",
        "    # convert the targets list to a 2d array\n",
        "    targets = np.array(targets_list, ndmin = 2).T\n",
        "    # calculate signals into the hidden layer\n",
        "    hidden_inputs = np.dot(self.wih, inputs)\n",
        "    # calculate signals emerging from the hidden layer\n",
        "    hidden_outputs = self.activation_function(hidden_inputs)\n",
        "    # calculate signals into final output layer\n",
        "    final_inputs = np.dot(self.who, hidden_outputs)\n",
        "    # calculate signals emerging from the final output layer\n",
        "    final_outputs = self.activation_function(final_inputs)\n",
        "\n",
        "    # output layer error is (target - actual)\n",
        "    output_errors = targets - final_outputs\n",
        "    # hidden layer error\n",
        "    hidden_errors = np.dot(self.who.T, output_errors)\n",
        "    # update the link weights between hidden & output layers\n",
        "    self.who += self.learnRate * np.dot((output_errors * final_outputs *\n",
        "      (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
        "    # update the link weights between input & hidden layers\n",
        "    self.wih += self.learnRate * np.dot((hidden_errors * hidden_outputs *\n",
        "      (1.0 - hidden_outputs)), np.transpose(inputs))\n",
        "    pass\n",
        "\n",
        "  # query the neural network\n",
        "  def query(self, inputs_list):\n",
        "    # convert inputs list to 2d array\n",
        "    inputs = np.array(inputs_list, ndmin=2).T\n",
        "    # calculate signals into hidden layer\n",
        "    hidden_inputs = np.dot(self.wih, inputs)\n",
        "    # calculate the signals emerging from hidden layer\n",
        "    hidden_outputs = self.activation_function(hidden_inputs)\n",
        "    # calculate signals into final output layer\n",
        "    final_inputs = np.dot(self.who, hidden_outputs)\n",
        "    # calculate the signals emerging from final output layer\n",
        "    final_outputs = self.activation_function(final_inputs)\n",
        "    return final_outputs\n"
      ],
      "metadata": {
        "id": "6vH55BhO_t1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is test code for the class"
      ],
      "metadata": {
        "id": "gqeaLUtuDdGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "inputNodes = 3\n",
        "hiddenNodes = 3\n",
        "outputNodes = 3\n",
        "learningRate = 0.3\n",
        "n = neuralNetwork(inputNodes, hiddenNodes, outputNodes, learningRate)\n",
        "n.query([1.0, 0.5, -1.5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8d69MCkDaz9",
        "outputId": "6c6975da-67b6-442e-94b3-49afa4ed165c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.48900539],\n",
              "       [0.60154223],\n",
              "       [0.72015492]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There is a collection of images of handwritten numbers used by artificial intelligence researchers as a popular set to test their latest ideas and algorithms.\n",
        "\n",
        "We will use it as well. The collection is broken up into two subsets:\n",
        "\n",
        "> http://www.pjreddie.com/media/files/mnist_train.csv\n",
        "\n",
        "> The training set​ is the set of 60,000​ labelled examples used to train the neural network. Labelled​ means the inputs come with the desired output, that is, what the answer should be.\n",
        "\n",
        "> http://www.pjreddie.com/media/files/mnist_test.csv\n",
        "\n",
        "> The smaller test set​ of 10,000​ is used to see how well our idea or algorithm works. This too contains the correct labels so we can check to see if our own neural network got the answer right or not."
      ],
      "metadata": {
        "id": "vKstFGcWH6nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start our experiments with subsets of the subsets\n",
        "\n",
        "> https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv\n",
        "\n",
        "> 100 records from the training subset\n",
        "\n",
        "> https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_test_10.csv\n",
        "\n",
        "\n",
        "> 10 records from the test subset"
      ],
      "metadata": {
        "id": "Vk8mMISOJKHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will drag and drop the files we want from  our storage drive (we can use the desktop or the cloud for storage) into the goggle colab file icon (left side of the colab screen). We will need to repeat this process each time that we access the workbook/notebook\n",
        "\n",
        "> This process yields the following file paths:\n",
        "\n",
        "  > * content/mnist_test_10.csv\n",
        "\n",
        "  > * content/minist_train_100.csv"
      ],
      "metadata": {
        "id": "Oum19PzOxxXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the test file"
      ],
      "metadata": {
        "id": "2sblEfT2qzWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = open('mnist_test_10.csv', 'r')\n",
        "data_list = data_file.readlines()\n",
        "data_file.close()\n",
        "\n",
        "len(data_list)\n",
        "\n",
        "data_list[0]\n",
        "\n",
        "#max(data_list[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "G6FGvdPHqaA7",
        "outputId": "cdf60922-2d37-44b1-8e85-7b9dfd464885"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,185,159,151,60,36,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,222,254,254,254,254,241,198,198,198,198,198,198,198,198,170,52,0,0,0,0,0,0,0,0,0,0,0,0,67,114,72,114,163,227,254,225,254,254,254,250,229,254,254,140,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,66,14,67,67,67,59,21,236,254,106,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,253,209,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,233,255,83,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,129,254,238,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,59,249,254,62,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,133,254,187,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,205,248,58,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,126,254,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,75,251,240,57,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,221,254,166,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,203,254,219,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,38,254,254,77,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,224,254,115,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,133,254,254,52,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,61,242,254,254,52,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,121,254,254,219,40,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,121,254,207,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at one of the numbers in the test file\n",
        "\n",
        "> Remember that each pixel in this 28 x 28 pixel array has a different value ranging from 0 to 255.\n",
        "\n",
        "> Conceptually there is a grid of pixels overlaying a defined space (height, width) in which a number has been hand written.  Battleship, chess board, grid squares ... take your pick\n",
        "\n",
        "> First three lines of the code snippet serve to import the libraries that we will need to accomplish the task\n",
        "\n",
        "> Fourth line of code takes the first record from data_list[0] and splits the string by commas\n",
        "\n",
        "> Fifth line has several parts that will work on the all_values data\n",
        "\n",
        "  > * [1:] takes everything except for the first element of the list\n",
        "\n",
        "  > * numpy.asfarray() converts text strings into real numbers and creates an array of them\n",
        "\n",
        "  > * .reshape((28,28)) wraps the list of numbers around every 28 elements in order to make a square 28 by 28 matrix\n",
        "\n",
        "  > * the results of all of this work is a variable called image_array\n",
        "\n",
        "> Sixth line does the following\n",
        "\n",
        "  > *  plots the image_array using the imshow() function\n",
        "\n",
        "  > * uses a blue colour palette via the cmap='Blues'\n",
        "\n"
      ],
      "metadata": {
        "id": "-3ZmzRipsw9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "all_values = data_list[0].split(',')\n",
        "image_array = np.asfarray(all_values[1:]).reshape(28,28)\n",
        "plt.imshow(image_array, cmap='Blues', interpolation='None')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nrC5bLJfs6ru",
        "outputId": "e168e023-df76-4773-c74c-7c9dd30bd7bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdac3c41f00>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa/0lEQVR4nO3df3BU9f3v8Vf4kRU0WQwh2awsNICCFYi3VNIMSlEyQHovA8Ifot654PULAw3eQmp10qugbb+TFr9jGb0Upt9pSZ0RtMxXYPR+B68EE8Y20AHh5mJtSnKjwJAETW92Q5DAkM/9g3HblQCeZTfv7PJ8zJwZsns+OW9Pd/LsSTYnGc45JwAA+tkg6wEAADcnAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwMsR7gq3p7e3X69GllZWUpIyPDehwAgEfOOXV1dSkYDGrQoKtf5wy4AJ0+fVqhUMh6DADADTp58qRGjx591ecHXICysrIkSU0tJ5WVnW08DQDAq65IRBMKQ9Gv51eTtABt2rRJL730ktra2lRUVKRXX31V06dPv+66L7/tlpWdrWwCBAAp63o/RknKmxDefPNNVVRUaP369frwww9VVFSkuXPn6syZM8k4HAAgBSUlQC+//LKWL1+uJ554Qt/85je1ZcsWDR8+XL/97W+TcTgAQApKeIAuXLigw4cPq7S09O8HGTRIpaWlqq+vv2L/np4eRSKRmA0AkP4SHqDPP/9cly5dUn5+fszj+fn5amtru2L/qqoq+f3+6MY74ADg5mD+i6iVlZUKh8PR7eTJk9YjAQD6QcLfBZebm6vBgwervb095vH29nYFAoEr9vf5fPL5fIkeAwAwwCX8CigzM1PTpk1TTU1N9LHe3l7V1NSopKQk0YcDAKSopPweUEVFhZYuXapvf/vbmj59ujZu3Kju7m498cQTyTgcACAFJSVAjzzyiD777DOtW7dObW1tuvfee7Vnz54r3pgAALh5ZTjnnPUQ/ygSicjv96u9I8ydEAAgBUUiEeWP9CscvvbXcfN3wQEAbk4ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEwgP0wgsvKCMjI2abNGlSog8DAEhxQ5LxSe+55x7t3bv37wcZkpTDAABSWFLKMGTIEAUCgWR8agBAmkjKz4COHz+uYDCocePG6fHHH9eJEyeuum9PT48ikUjMBgBIfwkPUHFxsaqrq7Vnzx5t3rxZLS0teuCBB9TV1dXn/lVVVfL7/dEtFAoleiQAwACU4ZxzyTxAZ2enxo4dq5dffllPPvnkFc/39PSop6cn+nEkElEoFFJ7R1jZ2dnJHA0AkASRSET5I/0Kh6/9dTzp7w4YMWKE7rrrLjU1NfX5vM/nk8/nS/YYAIABJum/B3T27Fk1NzeroKAg2YcCAKSQhAfo6aefVl1dnT755BP98Y9/1MMPP6zBgwfr0UcfTfShAAApLOHfgjt16pQeffRRdXR0aNSoUbr//vt14MABjRo1KtGHAgCksIQH6I033kj0pwQApCHuBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmEj6H6RD//qfH7V6XvPftzfEdaw77vD+F2uH+7y/5J6bfafnNTm3ZXpeI0mhkcPjWgfAO66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIK7YaeZ//yTf/e+qOVoXMf6NK5V3u3dEseirNy4jjVyclFc69B/Rodu97xm86P/Ia5j3R3HHd/x9XEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4Gakaebdqoc9r3n/k5lxHWtGHDeF/MPJ/+d5zb6Pznhe86d9DZ7XSFJHfY33RWOmeF9z4v94X9OfhmR6XzPqG97XtP7V85KOeu+H+ZfRfu+LJP1myb1xrcPXwxUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5GmmenjcvplTbzuvzPX85pnH7rT85rIP033vEaS/tp61vOau4NZntd8dDrieU1/umXIYM9rQiOHeV4zbvEvPa/R3055XjItlO39OEg6roAAACYIEADAhOcA7d+/X/Pnz1cwGFRGRoZ27doV87xzTuvWrVNBQYGGDRum0tJSHT9+PFHzAgDShOcAdXd3q6ioSJs2berz+Q0bNuiVV17Rli1bdPDgQd16662aO3euzp8/f8PDAgDSh+c3IZSVlamsrKzP55xz2rhxo5577jktWLBAkvTaa68pPz9fu3bt0pIlS25sWgBA2kjoz4BaWlrU1tam0tLS6GN+v1/FxcWqr+/77+j29PQoEonEbACA9JfQALW1tUmS8vPzYx7Pz8+PPvdVVVVV8vv90S0UCiVyJADAAGX+LrjKykqFw+HodvLkSeuRAAD9IKEBCgQCkqT29vaYx9vb26PPfZXP51N2dnbMBgBIfwkNUGFhoQKBgGpqaqKPRSIRHTx4UCUlJYk8FAAgxXl+F9zZs2fV1NQU/bilpUVHjx5VTk6OxowZozVr1uhnP/uZ7rzzThUWFur5559XMBjUwoULEzk3ACDFeQ7QoUOH9OCDD0Y/rqiokCQtXbpU1dXVeuaZZ9Td3a0VK1aos7NT999/v/bs2aNbbrklcVMDAFJehnPOWQ/xjyKRiPx+v9o7wvw8CEgh//5Rq+c1j//Xn3tec+sU79/O/+iVxZ7XSJJ/+NC41t3sIpGI8kf6FQ5f++u4+bvgAAA3JwIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjw/OcYAKS/v5294HnN489s836g3kuel/x67Xc9r+Gu1gMTV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRgrgCj+rafK+6LNPvK+5Peh5ycTcLO/HwYDEFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKbkQJprOFEOK51WzdsTfAkfXv/19/3vGZ8/m1JmAQWuAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwM1Igjf360Mn4Fl4873lJqPQ/el4zOZTteQ3SB1dAAAATBAgAYMJzgPbv36/58+crGAwqIyNDu3btinl+2bJlysjIiNnmzZuXqHkBAGnCc4C6u7tVVFSkTZs2XXWfefPmqbW1Nbpt3779hoYEAKQfz29CKCsrU1lZ2TX38fl8CgQCcQ8FAEh/SfkZUG1trfLy8jRx4kStWrVKHR0dV923p6dHkUgkZgMApL+EB2jevHl67bXXVFNTo1/84heqq6tTWVmZLl261Of+VVVV8vv90S0UCiV6JADAAJTw3wNasmRJ9N9TpkzR1KlTNX78eNXW1mr27NlX7F9ZWamKiorox5FIhAgBwE0g6W/DHjdunHJzc9XU1NTn8z6fT9nZ2TEbACD9JT1Ap06dUkdHhwoKCpJ9KABACvH8LbizZ8/GXM20tLTo6NGjysnJUU5Ojl588UUtXrxYgUBAzc3NeuaZZzRhwgTNnTs3oYMDAFKb5wAdOnRIDz74YPTjL39+s3TpUm3evFkNDQ363e9+p87OTgWDQc2ZM0c//elP5fP5Ejc1ACDleQ7QrFmz5Jy76vPvvvvuDQ0EoG/nL/b9TtJreft/fRzfwTKHeV7y+soSz2uGDOZuYDcz/tcHAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiYT/SW4AybHu3b96XhP5cH9cxxpXNt/zmilj/HEdCzcvroAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBQwUNv4mec1//rP/+r9QNl53tdI+t0/Fce1DvCCKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3IwVuUPjcRc9rFq1/2/uBLnk/zrT/NMv7cSRNDvnjWgd4wRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5EC/+BSr/O8pujp3Z7XuP97xPOawROmeV5T/V++7XkN0F+4AgIAmCBAAAATngJUVVWl++67T1lZWcrLy9PChQvV2NgYs8/58+dVXl6ukSNH6rbbbtPixYvV3t6e0KEBAKnPU4Dq6upUXl6uAwcO6L333tPFixc1Z84cdXd3R/dZu3at3n77be3YsUN1dXU6ffq0Fi1alPDBAQCpzdObEPbs2RPzcXV1tfLy8nT48GHNnDlT4XBYv/nNb7Rt2zY99NBDkqStW7fq7rvv1oEDB/Sd73wncZMDAFLaDf0MKBwOS5JycnIkSYcPH9bFixdVWloa3WfSpEkaM2aM6uvr+/wcPT09ikQiMRsAIP3FHaDe3l6tWbNGM2bM0OTJkyVJbW1tyszM1IgRI2L2zc/PV1tbW5+fp6qqSn6/P7qFQqF4RwIApJC4A1ReXq5jx47pjTfeuKEBKisrFQ6Ho9vJkydv6PMBAFJDXL+Iunr1ar3zzjvav3+/Ro8eHX08EAjowoUL6uzsjLkKam9vVyAQ6PNz+Xw++Xy+eMYAAKQwT1dAzjmtXr1aO3fu1L59+1RYWBjz/LRp0zR06FDV1NREH2tsbNSJEydUUlKSmIkBAGnB0xVQeXm5tm3bpt27dysrKyv6cx2/369hw4bJ7/frySefVEVFhXJycpSdna2nnnpKJSUlvAMOABDDU4A2b94sSZo1a1bM41u3btWyZcskSb/85S81aNAgLV68WD09PZo7d65+9atfJWRYAED6yHDOeb/7YhJFIhH5/X61d4SVnZ1tPQ5uMic+P+d5TdH3nk3CJFf6t9ee97zmoUl5SZgEuLZIJKL8kX6Fw9f+Os694AAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAirr+ICgx0rZ3n41pXtOr1BE/St2er/pvnNQ9OHJWESQA7XAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSnS0j/va4pv4acNiR3kKhZMyve8JiMjIwmTAHa4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUgx4//vTTs9rXv8fOxI/CICE4goIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUgx4P3bx+3eF539W+IHuYrBE6Z5XjMsc3ASJgFSC1dAAAATBAgAYMJTgKqqqnTfffcpKytLeXl5WrhwoRobG2P2mTVrljIyMmK2lStXJnRoAEDq8xSguro6lZeX68CBA3rvvfd08eJFzZkzR93d3TH7LV++XK2trdFtw4YNCR0aAJD6PL0JYc+ePTEfV1dXKy8vT4cPH9bMmTOjjw8fPlyBQCAxEwIA0tIN/QwoHA5LknJycmIef/3115Wbm6vJkyersrJS586du+rn6OnpUSQSidkAAOkv7rdh9/b2as2aNZoxY4YmT54cffyxxx7T2LFjFQwG1dDQoGeffVaNjY166623+vw8VVVVevHFF+MdAwCQouIOUHl5uY4dO6YPPvgg5vEVK1ZE/z1lyhQVFBRo9uzZam5u1vjx46/4PJWVlaqoqIh+HIlEFAqF4h0LAJAi4grQ6tWr9c4772j//v0aPXr0NfctLi6WJDU1NfUZIJ/PJ5/PF88YAIAU5ilAzjk99dRT2rlzp2pra1VYWHjdNUePHpUkFRQUxDUgACA9eQpQeXm5tm3bpt27dysrK0ttbW2SJL/fr2HDhqm5uVnbtm3T9773PY0cOVINDQ1au3atZs6cqalTpyblPwAAkJo8BWjz5s2SLv+y6T/aunWrli1bpszMTO3du1cbN25Ud3e3QqGQFi9erOeeey5hAwMA0oPnb8FdSygUUl1d3Q0NBAC4OXA3bOAf3Hbv/Z7XHNu4yPMa//ChntcA6YabkQIATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJjLc9W5x3c8ikYj8fr/aO8LKzs62HgcA4FEkElH+SL/C4Wt/HecKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIkh1gN81Ze3puuKRIwnAQDE48uv39e71eiAC1BXV5ckaUJhyHgSAMCN6Orqkt/vv+rzA+5u2L29vTp9+rSysrKUkZER81wkElEoFNLJkydv6jtlcx4u4zxcxnm4jPNw2UA4D845dXV1KRgMatCgq/+kZ8BdAQ0aNEijR4++5j7Z2dk39QvsS5yHyzgPl3EeLuM8XGZ9Hq515fMl3oQAADBBgAAAJlIqQD6fT+vXr5fP57MexRTn4TLOw2Wch8s4D5el0nkYcG9CAADcHFLqCggAkD4IEADABAECAJggQAAAEykToE2bNukb3/iGbrnlFhUXF+tPf/qT9Uj97oUXXlBGRkbMNmnSJOuxkm7//v2aP3++gsGgMjIytGvXrpjnnXNat26dCgoKNGzYMJWWlur48eM2wybR9c7DsmXLrnh9zJs3z2bYJKmqqtJ9992nrKws5eXlaeHChWpsbIzZ5/z58yovL9fIkSN12223afHixWpvbzeaODm+znmYNWvWFa+HlStXGk3ct5QI0JtvvqmKigqtX79eH374oYqKijR37lydOXPGerR+d88996i1tTW6ffDBB9YjJV13d7eKioq0adOmPp/fsGGDXnnlFW3ZskUHDx7Urbfeqrlz5+r8+fP9PGlyXe88SNK8efNiXh/bt2/vxwmTr66uTuXl5Tpw4IDee+89Xbx4UXPmzFF3d3d0n7Vr1+rtt9/Wjh07VFdXp9OnT2vRokWGUyfe1zkPkrR8+fKY18OGDRuMJr4KlwKmT5/uysvLox9funTJBYNBV1VVZThV/1u/fr0rKiqyHsOUJLdz587ox729vS4QCLiXXnop+lhnZ6fz+Xxu+/btBhP2j6+eB+ecW7p0qVuwYIHJPFbOnDnjJLm6ujrn3OX/7YcOHep27NgR3efjjz92klx9fb3VmEn31fPgnHPf/e533Q9+8AO7ob6GAX8FdOHCBR0+fFilpaXRxwYNGqTS0lLV19cbTmbj+PHjCgaDGjdunB5//HGdOHHCeiRTLS0tamtri3l9+P1+FRcX35Svj9raWuXl5WnixIlatWqVOjo6rEdKqnA4LEnKycmRJB0+fFgXL16MeT1MmjRJY8aMSevXw1fPw5def/115ebmavLkyaqsrNS5c+csxruqAXcz0q/6/PPPdenSJeXn58c8np+fr7/85S9GU9koLi5WdXW1Jk6cqNbWVr344ot64IEHdOzYMWVlZVmPZ6KtrU2S+nx9fPnczWLevHlatGiRCgsL1dzcrB//+McqKytTfX29Bg8ebD1ewvX29mrNmjWaMWOGJk+eLOny6yEzM1MjRoyI2TedXw99nQdJeuyxxzR27FgFg0E1NDTo2WefVWNjo9566y3DaWMN+ADh78rKyqL/njp1qoqLizV27Fj9/ve/15NPPmk4GQaCJUuWRP89ZcoUTZ06VePHj1dtba1mz55tOFlylJeX69ixYzfFz0Gv5WrnYcWKFdF/T5kyRQUFBZo9e7aam5s1fvz4/h6zTwP+W3C5ubkaPHjwFe9iaW9vVyAQMJpqYBgxYoTuuusuNTU1WY9i5svXAK+PK40bN065ublp+fpYvXq13nnnHb3//vsxf74lEAjowoUL6uzsjNk/XV8PVzsPfSkuLpakAfV6GPAByszM1LRp01RTUxN9rLe3VzU1NSopKTGczN7Zs2fV3NysgoIC61HMFBYWKhAIxLw+IpGIDh48eNO/Pk6dOqWOjo60en0457R69Wrt3LlT+/btU2FhYczz06ZN09ChQ2NeD42NjTpx4kRavR6udx76cvToUUkaWK8H63dBfB1vvPGG8/l8rrq62v35z392K1ascCNGjHBtbW3Wo/WrH/7wh662tta1tLS4P/zhD660tNTl5ua6M2fOWI+WVF1dXe7IkSPuyJEjTpJ7+eWX3ZEjR9ynn37qnHPu5z//uRsxYoTbvXu3a2hocAsWLHCFhYXuiy++MJ48sa51Hrq6utzTTz/t6uvrXUtLi9u7d6/71re+5e688053/vx569ETZtWqVc7v97va2lrX2toa3c6dOxfdZ+XKlW7MmDFu37597tChQ66kpMSVlJQYTp141zsPTU1N7ic/+Yk7dOiQa2lpcbt373bjxo1zM2fONJ48VkoEyDnnXn31VTdmzBiXmZnppk+f7g4cOGA9Ur975JFHXEFBgcvMzHR33HGHe+SRR1xTU5P1WEn3/vvvO0lXbEuXLnXOXX4r9vPPP+/y8/Odz+dzs2fPdo2NjbZDJ8G1zsO5c+fcnDlz3KhRo9zQoUPd2LFj3fLly9Pu/6T19d8vyW3dujW6zxdffOG+//3vu9tvv90NHz7cPfzww661tdVu6CS43nk4ceKEmzlzpsvJyXE+n89NmDDB/ehHP3LhcNh28K/gzzEAAEwM+J8BAQDSEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABg4v8D1lVqYIBTWloAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we are going to work on prepping our data for processing by scaling it.\n",
        "\n",
        "> Pixel values range from 0 to 255\n",
        "\n",
        "> Neural networks work best with values that range from 0.01 to 0.99\n",
        "\n",
        "> * input values of 0.0 kill weight updates and are only approached by, but not reached by, the logistic function\n",
        "\n",
        "> * output values of 1.0 are only approached by, but not reached by, the logistic function"
      ],
      "metadata": {
        "id": "gC4rl5V47gTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_input = (np.asfarray(all_values[1:])/255.0 * 0.99) + 0.01\n",
        "print(scaled_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPFphNet8hia",
        "outputId": "0eaf43f7-c733-48d5-ea68-3f8a3b6d30da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.33611765 0.72823529\n",
            " 0.62729412 0.59623529 0.24294118 0.14976471 0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.87188235 0.99611765 0.99611765 0.99611765\n",
            " 0.99611765 0.94564706 0.77870588 0.77870588 0.77870588 0.77870588\n",
            " 0.77870588 0.77870588 0.77870588 0.77870588 0.67       0.21188235\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.27011765 0.45258824 0.28952941 0.45258824 0.64282353 0.89129412\n",
            " 0.99611765 0.88352941 0.99611765 0.99611765 0.99611765 0.98058824\n",
            " 0.89905882 0.99611765 0.99611765 0.55352941 0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.076      0.26623529 0.06435294\n",
            " 0.27011765 0.27011765 0.27011765 0.23905882 0.09152941 0.92623529\n",
            " 0.99611765 0.42152941 0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.33223529 0.99223529 0.82141176 0.07988235\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.09541176\n",
            " 0.91458824 1.         0.33223529 0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.51082353 0.99611765 0.934\n",
            " 0.18082353 0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.23905882 0.97670588 0.99611765 0.25070588 0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.52635294 0.99611765\n",
            " 0.736      0.02941176 0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.04494118 0.80588235 0.97282353 0.23517647 0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.49917647\n",
            " 0.99611765 0.71658824 0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.30117647 0.98447059 0.94176471 0.23129412\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.08376471\n",
            " 0.868      0.99611765 0.65447059 0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.02164706 0.79811765 0.99611765 0.86023529\n",
            " 0.14588235 0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.15752941 0.99611765 0.99611765 0.30894118 0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.13035294 0.87964706 0.99611765\n",
            " 0.45647059 0.01388235 0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.52635294 0.99611765 0.99611765 0.21188235 0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.24682353 0.94952941\n",
            " 0.99611765 0.99611765 0.21188235 0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.47976471 0.99611765 0.99611765 0.86023529\n",
            " 0.16529412 0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.47976471 0.99611765 0.81364706 0.07988235 0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01       0.01       0.01\n",
            " 0.01       0.01       0.01       0.01      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is for the neural network to classify the data (images) and assign the correct label. This means that:\n",
        "\n",
        "> Each label will be one of ten numbers ranging from 0 to 9\n",
        "\n",
        "> The labels will use target arrays\n",
        "\n",
        "> We will use ten nodes, one for each number\n",
        "\n",
        "We will use a four line code snippet to take a look at how this works\n",
        "\n",
        "> the first line sets the number of output nodes to 10\n",
        "\n",
        "> the second line uses the .zero function to create an array filled with zeros and adds the value 0.01 to account for the logistic function's aversion to 0.0\n",
        "\n",
        "> the third line takes the first element of the MNIST dataset, which is the training label, and converts the string into an integer. We do this because we read the record from the source file as a text string. We next add 0.99 to account for the logistic function's aversion to 1.0"
      ],
      "metadata": {
        "id": "GU12dnJs_VmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 output node example\n",
        "\n",
        "outNodes = 10\n",
        "targets = np.zeros(outNodes) + 0.01\n",
        "targets[int(all_values[0])] = 0.99\n",
        "\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OC5kJCDAh-q",
        "outputId": "a7470841-0bb8-4f7e-f950-a92c3809f6cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99 0.01 0.01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will scale up our work to account for the 28 x 28 = 784 pixel system that we are using"
      ],
      "metadata": {
        "id": "US-lnADJDivX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to prep and input data into the neural network for training\n",
        "\n",
        "# define the inputs needed to talk to the neural network class\n",
        "inputNodes = 784\n",
        "hiddenNodes = 100\n",
        "outputNodes = 10\n",
        "learningRate = 0.3\n",
        "n = neuralNetwork(inputNodes, hiddenNodes, outputNodes, learningRate)\n",
        "\n",
        "# load the MNIST training data csv file into a list\n",
        "training_data_file = open('mnist_dataset/mnist_train_100.csv', 'r')\n",
        "training_data_list = training_data_file.readlines()\n",
        "training_data_file.close()\n",
        "\n",
        "# go through all records in the training data set\n",
        "for record in training_data_list:\n",
        "  # split the record by the ',' commas\n",
        "  all_values = record.split(',')\n",
        "  # scale and shift the inputs\n",
        "  inputs = (np.asfarray(all_values[1:])/255.0 * 0.99) + 0.01)\n",
        "  # create the target output values (all 0.01, except for the desired label which is 0.99)\n",
        "  targets_784 = np.zeros(outputNodes) + 0.01\n",
        "  # all_values[0] is the target label for this record\n",
        "  targets_784[int(all_values[0])] = 0.99\n",
        "  n.train(inputs, targets)\n",
        "  pass"
      ],
      "metadata": {
        "id": "qIeWg2fOEAqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's update the class with this work"
      ],
      "metadata": {
        "id": "G0-YTq6FIr9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy.special\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# draft class definition for a neural network\n",
        "class neuralNetwork:\n",
        "\n",
        "  # intialize the neural network\n",
        "  def __init__(self, inputNodes, hiddenNodes, outputNodes, learningRate):\n",
        "    # layers and learning rates\n",
        "    self.iNodes = inputNodes\n",
        "    self.hNodes = hiddenNodes\n",
        "    self.oNodes = outputNodes\n",
        "    self.learnRate = learningRate\n",
        "    # link weights connecting the layers via matrices\n",
        "    self.wih = np.random.normal(0.0, pow(self.iNodes, -0.5), (self.hNodes, self.iNodes))\n",
        "    self.who = np.random.normal(0.0, pow(self.hNodes, -0.5), (self.oNodes, self.hNodes))\n",
        "    # sigmoid activation function\n",
        "    self.activation_function = lambda x: scipy.special.expit(x)\n",
        "    pass\n",
        "\n",
        "  # train the neural network\n",
        "  def train(self, inputs_list, targets_list):\n",
        "    # convert the inputs list to a 2d array\n",
        "    inputs = np.array(inputs_list, ndmin = 2).T\n",
        "    # convert the targets list to a 2d array\n",
        "    targets = np.array(targets_list, ndmin = 2).T\n",
        "    # calculate signals into the hidden layer\n",
        "    hidden_inputs = np.dot(self.wih, inputs)\n",
        "    # calculate signals emerging from the hidden layer\n",
        "    hidden_outputs = self.activation_function(hidden_inputs)\n",
        "    # calculate signals into final output layer\n",
        "    final_inputs = np.dot(self.who, hidden_outputs)\n",
        "    # calculate signals emerging from the final output layer\n",
        "    final_outputs = self.activation_function(final_inputs)\n",
        "\n",
        "    # output layer error is (target - actual)\n",
        "    output_errors = targets - final_outputs\n",
        "    # hidden layer error\n",
        "    hidden_errors = np.dot(self.who.T, output_errors)\n",
        "    # update the link weights between hidden & output layers\n",
        "    self.who += self.learnRate * np.dot((output_errors * final_outputs *\n",
        "      (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
        "    # update the link weights between input & hidden layers\n",
        "    self.wih += self.learnRate * np.dot((hidden_errors * hidden_outputs *\n",
        "      (1.0 - hidden_outputs)), np.transpose(inputs))\n",
        "    pass\n",
        "\n",
        "  # query the neural network\n",
        "  def query(self, inputs_list):\n",
        "    # convert inputs list to 2d array\n",
        "    inputs = np.array(inputs_list, ndmin=2).T\n",
        "    # calculate signals into hidden layer\n",
        "    hidden_inputs = np.dot(self.wih, inputs)\n",
        "    # calculate the signals emerging from hidden layer\n",
        "    hidden_outputs = self.activation_function(hidden_inputs)\n",
        "    # calculate signals into final output layer\n",
        "    final_inputs = np.dot(self.who, hidden_outputs)\n",
        "    # calculate the signals emerging from final output layer\n",
        "    final_outputs = self.activation_function(final_inputs)\n",
        "    return final_outputs\n",
        "\n",
        "# code to prep and input data into the neural network for training\n",
        "\n",
        "# define the inputs needed to talk to the neural network class\n",
        "inputNodes = 784\n",
        "hiddenNodes = 100\n",
        "outputNodes = 10\n",
        "learningRate = 0.3\n",
        "n = neuralNetwork(inputNodes, hiddenNodes, outputNodes, learningRate)\n",
        "\n",
        "# load the MNIST training data csv file into a list\n",
        "training_data_file = open('mnist_train_100.csv', 'r')\n",
        "training_data_list = training_data_file.readlines()\n",
        "training_data_file.close()\n",
        "\n",
        "# go through all records in the training data set\n",
        "for record in training_data_list:\n",
        "  # split the record by the ',' commas\n",
        "  all_values = record.split(',')\n",
        "  # scale and shift the inputs\n",
        "  inputs = (np.asfarray(all_values[1:])/255.0 * 0.99) + 0.01\n",
        "  # create the target output values (all 0.01, except for the desired label which is 0.99)\n",
        "  targets_784 = np.zeros(outputNodes) + 0.01\n",
        "  # all_values[0] is the target label for this record\n",
        "  targets_784[int(all_values[0])] = 0.99\n",
        "  n.train(inputs, targets)\n",
        "  pass"
      ],
      "metadata": {
        "id": "lRQTEtQ-I8xi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build and run a test of the network"
      ],
      "metadata": {
        "id": "Gp_dW9yDJNJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MNIST test data csv file into a list\n",
        "test_data_file = open('mnist_test_10.csv', )\n",
        "test_data_list = test_data_file.readlines()\n",
        "test_data_file.close()\n",
        "\n",
        "# get the first test record\n",
        "all_values = test_data_list[0].split(',')\n",
        "# print the label\n",
        "print('the data label is ', all_values[0], '\\n')\n",
        "# plot the image\n",
        "image_array = np.asfarray(all_values[1:]).reshape((28,28))\n",
        "plt.imshow(image_array, cmap='Blues', interpolation='None')\n",
        "# show the outputs from each of the ten nodes\n",
        "print('the array values are: \\n')\n",
        "print(n.query((np.asfarray(all_values[1:])/255.0 * 0.99 +0.01)))\n",
        "print('\\n the handwritten image is \\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "R4uSkMOtJSJ8",
        "outputId": "69b69e7b-ffca-4c7f-8dbf-3ed57be93da8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the data label is  7 \n",
            "\n",
            "the array values are: \n",
            "\n",
            "[[0.02298638]\n",
            " [0.0223923 ]\n",
            " [0.02283003]\n",
            " [0.02210305]\n",
            " [0.02176126]\n",
            " [0.02105039]\n",
            " [0.02214755]\n",
            " [0.9808243 ]\n",
            " [0.02179456]\n",
            " [0.02258272]]\n",
            "\n",
            " the handwritten image is \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa/0lEQVR4nO3df3BU9f3v8Vf4kRU0WQwh2awsNICCFYi3VNIMSlEyQHovA8Ifot654PULAw3eQmp10qugbb+TFr9jGb0Upt9pSZ0RtMxXYPR+B68EE8Y20AHh5mJtSnKjwJAETW92Q5DAkM/9g3HblQCeZTfv7PJ8zJwZsns+OW9Pd/LsSTYnGc45JwAA+tkg6wEAADcnAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwMsR7gq3p7e3X69GllZWUpIyPDehwAgEfOOXV1dSkYDGrQoKtf5wy4AJ0+fVqhUMh6DADADTp58qRGjx591ecHXICysrIkSU0tJ5WVnW08DQDAq65IRBMKQ9Gv51eTtABt2rRJL730ktra2lRUVKRXX31V06dPv+66L7/tlpWdrWwCBAAp63o/RknKmxDefPNNVVRUaP369frwww9VVFSkuXPn6syZM8k4HAAgBSUlQC+//LKWL1+uJ554Qt/85je1ZcsWDR8+XL/97W+TcTgAQApKeIAuXLigw4cPq7S09O8HGTRIpaWlqq+vv2L/np4eRSKRmA0AkP4SHqDPP/9cly5dUn5+fszj+fn5amtru2L/qqoq+f3+6MY74ADg5mD+i6iVlZUKh8PR7eTJk9YjAQD6QcLfBZebm6vBgwervb095vH29nYFAoEr9vf5fPL5fIkeAwAwwCX8CigzM1PTpk1TTU1N9LHe3l7V1NSopKQk0YcDAKSopPweUEVFhZYuXapvf/vbmj59ujZu3Kju7m498cQTyTgcACAFJSVAjzzyiD777DOtW7dObW1tuvfee7Vnz54r3pgAALh5ZTjnnPUQ/ygSicjv96u9I8ydEAAgBUUiEeWP9CscvvbXcfN3wQEAbk4ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEwgP0wgsvKCMjI2abNGlSog8DAEhxQ5LxSe+55x7t3bv37wcZkpTDAABSWFLKMGTIEAUCgWR8agBAmkjKz4COHz+uYDCocePG6fHHH9eJEyeuum9PT48ikUjMBgBIfwkPUHFxsaqrq7Vnzx5t3rxZLS0teuCBB9TV1dXn/lVVVfL7/dEtFAoleiQAwACU4ZxzyTxAZ2enxo4dq5dffllPPvnkFc/39PSop6cn+nEkElEoFFJ7R1jZ2dnJHA0AkASRSET5I/0Kh6/9dTzp7w4YMWKE7rrrLjU1NfX5vM/nk8/nS/YYAIABJum/B3T27Fk1NzeroKAg2YcCAKSQhAfo6aefVl1dnT755BP98Y9/1MMPP6zBgwfr0UcfTfShAAApLOHfgjt16pQeffRRdXR0aNSoUbr//vt14MABjRo1KtGHAgCksIQH6I033kj0pwQApCHuBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmEj6H6RD//qfH7V6XvPftzfEdaw77vD+F2uH+7y/5J6bfafnNTm3ZXpeI0mhkcPjWgfAO66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIK7YaeZ//yTf/e+qOVoXMf6NK5V3u3dEseirNy4jjVyclFc69B/Rodu97xm86P/Ia5j3R3HHd/x9XEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4Gakaebdqoc9r3n/k5lxHWtGHDeF/MPJ/+d5zb6Pznhe86d9DZ7XSFJHfY33RWOmeF9z4v94X9OfhmR6XzPqG97XtP7V85KOeu+H+ZfRfu+LJP1myb1xrcPXwxUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5GmmenjcvplTbzuvzPX85pnH7rT85rIP033vEaS/tp61vOau4NZntd8dDrieU1/umXIYM9rQiOHeV4zbvEvPa/R3055XjItlO39OEg6roAAACYIEADAhOcA7d+/X/Pnz1cwGFRGRoZ27doV87xzTuvWrVNBQYGGDRum0tJSHT9+PFHzAgDShOcAdXd3q6ioSJs2berz+Q0bNuiVV17Rli1bdPDgQd16662aO3euzp8/f8PDAgDSh+c3IZSVlamsrKzP55xz2rhxo5577jktWLBAkvTaa68pPz9fu3bt0pIlS25sWgBA2kjoz4BaWlrU1tam0tLS6GN+v1/FxcWqr+/77+j29PQoEonEbACA9JfQALW1tUmS8vPzYx7Pz8+PPvdVVVVV8vv90S0UCiVyJADAAGX+LrjKykqFw+HodvLkSeuRAAD9IKEBCgQCkqT29vaYx9vb26PPfZXP51N2dnbMBgBIfwkNUGFhoQKBgGpqaqKPRSIRHTx4UCUlJYk8FAAgxXl+F9zZs2fV1NQU/bilpUVHjx5VTk6OxowZozVr1uhnP/uZ7rzzThUWFur5559XMBjUwoULEzk3ACDFeQ7QoUOH9OCDD0Y/rqiokCQtXbpU1dXVeuaZZ9Td3a0VK1aos7NT999/v/bs2aNbbrklcVMDAFJehnPOWQ/xjyKRiPx+v9o7wvw8CEgh//5Rq+c1j//Xn3tec+sU79/O/+iVxZ7XSJJ/+NC41t3sIpGI8kf6FQ5f++u4+bvgAAA3JwIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjw/OcYAKS/v5294HnN489s836g3kuel/x67Xc9r+Gu1gMTV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRgrgCj+rafK+6LNPvK+5Peh5ycTcLO/HwYDEFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKbkQJprOFEOK51WzdsTfAkfXv/19/3vGZ8/m1JmAQWuAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwM1Igjf360Mn4Fl4873lJqPQ/el4zOZTteQ3SB1dAAAATBAgAYMJzgPbv36/58+crGAwqIyNDu3btinl+2bJlysjIiNnmzZuXqHkBAGnCc4C6u7tVVFSkTZs2XXWfefPmqbW1Nbpt3779hoYEAKQfz29CKCsrU1lZ2TX38fl8CgQCcQ8FAEh/SfkZUG1trfLy8jRx4kStWrVKHR0dV923p6dHkUgkZgMApL+EB2jevHl67bXXVFNTo1/84heqq6tTWVmZLl261Of+VVVV8vv90S0UCiV6JADAAJTw3wNasmRJ9N9TpkzR1KlTNX78eNXW1mr27NlX7F9ZWamKiorox5FIhAgBwE0g6W/DHjdunHJzc9XU1NTn8z6fT9nZ2TEbACD9JT1Ap06dUkdHhwoKCpJ9KABACvH8LbizZ8/GXM20tLTo6NGjysnJUU5Ojl588UUtXrxYgUBAzc3NeuaZZzRhwgTNnTs3oYMDAFKb5wAdOnRIDz74YPTjL39+s3TpUm3evFkNDQ363e9+p87OTgWDQc2ZM0c//elP5fP5Ejc1ACDleQ7QrFmz5Jy76vPvvvvuDQ0EoG/nL/b9TtJreft/fRzfwTKHeV7y+soSz2uGDOZuYDcz/tcHAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiYT/SW4AybHu3b96XhP5cH9cxxpXNt/zmilj/HEdCzcvroAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBQwUNv4mec1//rP/+r9QNl53tdI+t0/Fce1DvCCKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3IwVuUPjcRc9rFq1/2/uBLnk/zrT/NMv7cSRNDvnjWgd4wRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5EC/+BSr/O8pujp3Z7XuP97xPOawROmeV5T/V++7XkN0F+4AgIAmCBAAAATngJUVVWl++67T1lZWcrLy9PChQvV2NgYs8/58+dVXl6ukSNH6rbbbtPixYvV3t6e0KEBAKnPU4Dq6upUXl6uAwcO6L333tPFixc1Z84cdXd3R/dZu3at3n77be3YsUN1dXU6ffq0Fi1alPDBAQCpzdObEPbs2RPzcXV1tfLy8nT48GHNnDlT4XBYv/nNb7Rt2zY99NBDkqStW7fq7rvv1oEDB/Sd73wncZMDAFLaDf0MKBwOS5JycnIkSYcPH9bFixdVWloa3WfSpEkaM2aM6uvr+/wcPT09ikQiMRsAIP3FHaDe3l6tWbNGM2bM0OTJkyVJbW1tyszM1IgRI2L2zc/PV1tbW5+fp6qqSn6/P7qFQqF4RwIApJC4A1ReXq5jx47pjTfeuKEBKisrFQ6Ho9vJkydv6PMBAFJDXL+Iunr1ar3zzjvav3+/Ro8eHX08EAjowoUL6uzsjLkKam9vVyAQ6PNz+Xw++Xy+eMYAAKQwT1dAzjmtXr1aO3fu1L59+1RYWBjz/LRp0zR06FDV1NREH2tsbNSJEydUUlKSmIkBAGnB0xVQeXm5tm3bpt27dysrKyv6cx2/369hw4bJ7/frySefVEVFhXJycpSdna2nnnpKJSUlvAMOABDDU4A2b94sSZo1a1bM41u3btWyZcskSb/85S81aNAgLV68WD09PZo7d65+9atfJWRYAED6yHDOeb/7YhJFIhH5/X61d4SVnZ1tPQ5uMic+P+d5TdH3nk3CJFf6t9ee97zmoUl5SZgEuLZIJKL8kX6Fw9f+Os694AAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAirr+ICgx0rZ3n41pXtOr1BE/St2er/pvnNQ9OHJWESQA7XAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSnS0j/va4pv4acNiR3kKhZMyve8JiMjIwmTAHa4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUgx4//vTTs9rXv8fOxI/CICE4goIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUgx4P3bx+3eF539W+IHuYrBE6Z5XjMsc3ASJgFSC1dAAAATBAgAYMJTgKqqqnTfffcpKytLeXl5WrhwoRobG2P2mTVrljIyMmK2lStXJnRoAEDq8xSguro6lZeX68CBA3rvvfd08eJFzZkzR93d3TH7LV++XK2trdFtw4YNCR0aAJD6PL0JYc+ePTEfV1dXKy8vT4cPH9bMmTOjjw8fPlyBQCAxEwIA0tIN/QwoHA5LknJycmIef/3115Wbm6vJkyersrJS586du+rn6OnpUSQSidkAAOkv7rdh9/b2as2aNZoxY4YmT54cffyxxx7T2LFjFQwG1dDQoGeffVaNjY166623+vw8VVVVevHFF+MdAwCQouIOUHl5uY4dO6YPPvgg5vEVK1ZE/z1lyhQVFBRo9uzZam5u1vjx46/4PJWVlaqoqIh+HIlEFAqF4h0LAJAi4grQ6tWr9c4772j//v0aPXr0NfctLi6WJDU1NfUZIJ/PJ5/PF88YAIAU5ilAzjk99dRT2rlzp2pra1VYWHjdNUePHpUkFRQUxDUgACA9eQpQeXm5tm3bpt27dysrK0ttbW2SJL/fr2HDhqm5uVnbtm3T9773PY0cOVINDQ1au3atZs6cqalTpyblPwAAkJo8BWjz5s2SLv+y6T/aunWrli1bpszMTO3du1cbN25Ud3e3QqGQFi9erOeeey5hAwMA0oPnb8FdSygUUl1d3Q0NBAC4OXA3bOAf3Hbv/Z7XHNu4yPMa//ChntcA6YabkQIATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJjLc9W5x3c8ikYj8fr/aO8LKzs62HgcA4FEkElH+SL/C4Wt/HecKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIkh1gN81Ze3puuKRIwnAQDE48uv39e71eiAC1BXV5ckaUJhyHgSAMCN6Orqkt/vv+rzA+5u2L29vTp9+rSysrKUkZER81wkElEoFNLJkydv6jtlcx4u4zxcxnm4jPNw2UA4D845dXV1KRgMatCgq/+kZ8BdAQ0aNEijR4++5j7Z2dk39QvsS5yHyzgPl3EeLuM8XGZ9Hq515fMl3oQAADBBgAAAJlIqQD6fT+vXr5fP57MexRTn4TLOw2Wch8s4D5el0nkYcG9CAADcHFLqCggAkD4IEADABAECAJggQAAAEykToE2bNukb3/iGbrnlFhUXF+tPf/qT9Uj97oUXXlBGRkbMNmnSJOuxkm7//v2aP3++gsGgMjIytGvXrpjnnXNat26dCgoKNGzYMJWWlur48eM2wybR9c7DsmXLrnh9zJs3z2bYJKmqqtJ9992nrKws5eXlaeHChWpsbIzZ5/z58yovL9fIkSN12223afHixWpvbzeaODm+znmYNWvWFa+HlStXGk3ct5QI0JtvvqmKigqtX79eH374oYqKijR37lydOXPGerR+d88996i1tTW6ffDBB9YjJV13d7eKioq0adOmPp/fsGGDXnnlFW3ZskUHDx7Urbfeqrlz5+r8+fP9PGlyXe88SNK8efNiXh/bt2/vxwmTr66uTuXl5Tpw4IDee+89Xbx4UXPmzFF3d3d0n7Vr1+rtt9/Wjh07VFdXp9OnT2vRokWGUyfe1zkPkrR8+fKY18OGDRuMJr4KlwKmT5/uysvLox9funTJBYNBV1VVZThV/1u/fr0rKiqyHsOUJLdz587ox729vS4QCLiXXnop+lhnZ6fz+Xxu+/btBhP2j6+eB+ecW7p0qVuwYIHJPFbOnDnjJLm6ujrn3OX/7YcOHep27NgR3efjjz92klx9fb3VmEn31fPgnHPf/e533Q9+8AO7ob6GAX8FdOHCBR0+fFilpaXRxwYNGqTS0lLV19cbTmbj+PHjCgaDGjdunB5//HGdOHHCeiRTLS0tamtri3l9+P1+FRcX35Svj9raWuXl5WnixIlatWqVOjo6rEdKqnA4LEnKycmRJB0+fFgXL16MeT1MmjRJY8aMSevXw1fPw5def/115ebmavLkyaqsrNS5c+csxruqAXcz0q/6/PPPdenSJeXn58c8np+fr7/85S9GU9koLi5WdXW1Jk6cqNbWVr344ot64IEHdOzYMWVlZVmPZ6KtrU2S+nx9fPnczWLevHlatGiRCgsL1dzcrB//+McqKytTfX29Bg8ebD1ewvX29mrNmjWaMWOGJk+eLOny6yEzM1MjRoyI2TedXw99nQdJeuyxxzR27FgFg0E1NDTo2WefVWNjo9566y3DaWMN+ADh78rKyqL/njp1qoqLizV27Fj9/ve/15NPPmk4GQaCJUuWRP89ZcoUTZ06VePHj1dtba1mz55tOFlylJeX69ixYzfFz0Gv5WrnYcWKFdF/T5kyRQUFBZo9e7aam5s1fvz4/h6zTwP+W3C5ubkaPHjwFe9iaW9vVyAQMJpqYBgxYoTuuusuNTU1WY9i5svXAK+PK40bN065ublp+fpYvXq13nnnHb3//vsxf74lEAjowoUL6uzsjNk/XV8PVzsPfSkuLpakAfV6GPAByszM1LRp01RTUxN9rLe3VzU1NSopKTGczN7Zs2fV3NysgoIC61HMFBYWKhAIxLw+IpGIDh48eNO/Pk6dOqWOjo60en0457R69Wrt3LlT+/btU2FhYczz06ZN09ChQ2NeD42NjTpx4kRavR6udx76cvToUUkaWK8H63dBfB1vvPGG8/l8rrq62v35z392K1ascCNGjHBtbW3Wo/WrH/7wh662tta1tLS4P/zhD660tNTl5ua6M2fOWI+WVF1dXe7IkSPuyJEjTpJ7+eWX3ZEjR9ynn37qnHPu5z//uRsxYoTbvXu3a2hocAsWLHCFhYXuiy++MJ48sa51Hrq6utzTTz/t6uvrXUtLi9u7d6/71re+5e688053/vx569ETZtWqVc7v97va2lrX2toa3c6dOxfdZ+XKlW7MmDFu37597tChQ66kpMSVlJQYTp141zsPTU1N7ic/+Yk7dOiQa2lpcbt373bjxo1zM2fONJ48VkoEyDnnXn31VTdmzBiXmZnppk+f7g4cOGA9Ur975JFHXEFBgcvMzHR33HGHe+SRR1xTU5P1WEn3/vvvO0lXbEuXLnXOXX4r9vPPP+/y8/Odz+dzs2fPdo2NjbZDJ8G1zsO5c+fcnDlz3KhRo9zQoUPd2LFj3fLly9Pu/6T19d8vyW3dujW6zxdffOG+//3vu9tvv90NHz7cPfzww661tdVu6CS43nk4ceKEmzlzpsvJyXE+n89NmDDB/ehHP3LhcNh28K/gzzEAAEwM+J8BAQDSEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABg4v8D1lVqYIBTWloAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}